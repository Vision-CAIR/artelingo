{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d89bf05",
   "metadata": {},
   "source": [
    "## Testing a single head model on custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ignite.metrics import Accuracy, Loss, Fbeta, recall, precision\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from utils import load_dataset, tokenize_dataset, create_dataloader\n",
    "import os\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "ARTEMIS_EMOTIONS = ['amusement', 'awe', 'contentment', 'excitement',\n",
    "                'anger', 'disgust',  'fear', 'sadness', 'something else']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8cbb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_version = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_version, padding_side='right')\n",
    "\n",
    "data_root_path = 'dataset'\n",
    "model_root_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb88d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = glob.glob('single_models/xlm*')\n",
    "print(all_models)\n",
    "langs = ['english', 'arabic', 'chinese', 'all_langs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c1384",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in langs:\n",
    "    for model_path in all_models:\n",
    "        # loading the model\n",
    "        model_path = os.path.join(model_root_path, model_path)\n",
    "        model_name = '_'.join(model_path.split('/')[-1].split('_')[:2])\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        data_path = os.path.join(data_root_path, f'test_{lang}/test_{lang}.csv')\n",
    "        sentences, labels = load_dataset(data_path, ARTEMIS_EMOTIONS, split='test')\n",
    "        tokens, masks = tokenize_dataset(tokenizer, sentences)\n",
    "        dataloader = create_dataloader(tokens, masks, labels, batch_size=128, mode='test')\n",
    "        \n",
    "        # evaluation loop\n",
    "        print(f'========= {model_name} :: {lang} =========')\n",
    "        t = trange(len(dataloader), desc='ML')\n",
    "        model.eval()\n",
    "        metrics = {'Accuracy': Accuracy(), \n",
    "                   'Precision': precision.Precision(average=True), \n",
    "                   'Recall': recall.Recall(average=True), \n",
    "                   'F1': Fbeta(1)\n",
    "                  }\n",
    "        loss_avg = Loss(F.cross_entropy)\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "        loss_avg.reset()\n",
    "        for step, batch in zip(t, dataloader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            input_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=input_mask) \n",
    "                emo_loss = F.cross_entropy(outputs.logits, labels, reduction = 'mean')\n",
    "            for metric in metrics.values():\n",
    "                metric.update((outputs.logits, labels.argmax(dim=1))) \n",
    "            loss_avg.update((outputs.logits, labels.argmax(dim=1))) \n",
    "            t.set_description(f'ML (loss={loss_avg.compute():.5f})')\n",
    "        for n, metric in metrics.items():\n",
    "            print(f'   {n}: {metric.compute():.5f}')\n",
    "        print(f'   Loss:     {loss_avg.compute():.5f}')\n",
    "        print(f'==========================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('vad_bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d85a04265eabdb4818588eb8176f04cc8800e0221914484e3dd3bf29608e438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
