{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d89bf05",
   "metadata": {},
   "source": [
    "## Testing a single head model on custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ignite.metrics import Accuracy, Loss, Fbeta, recall, precision\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from utils import load_dataset, tokenize_dataset, create_dataloader\n",
    "import os\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "ARTEMIS_EMOTIONS = ['amusement', 'awe', 'contentment', 'excitement',\n",
    "                'anger', 'disgust',  'fear', 'sadness', 'something else']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8cbb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0792db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    'single_models/bert_chinese',\n",
    "    'single_models/bert_english',\n",
    "    'single_models/CAMeL_arabic',\n",
    "]\n",
    "model_names = [\n",
    "    'bert_chinese',\n",
    "    'bert_english',\n",
    "    'bert_arabic',\n",
    "]\n",
    "langs = [\n",
    "    'chinese',\n",
    "    'english',\n",
    "    'arabic',\n",
    "]\n",
    "bert_versions = [\n",
    "    'bert-base-chinese',\n",
    "    'bert-base-uncased',\n",
    "    'CAMeL-Lab/bert-base-arabic-camelbert-mix',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c1384",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang, model_path, model_name, bert_version in zip(langs, model_paths, model_names, bert_versions):\n",
    "    # loading the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_version, padding_side='right')\n",
    "    # loading the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_path = os.path.join(data_root_path, f'test_{lang}/test_{lang}.csv')\n",
    "    sentences, labels = load_dataset(data_path, ARTEMIS_EMOTIONS, split='test')\n",
    "    tokens, masks = tokenize_dataset(tokenizer, sentences)\n",
    "    dataloader = create_dataloader(tokens, masks, labels, batch_size=128, mode='test')\n",
    "\n",
    "    # evaluation loop\n",
    "    print(f'========= {model_name} :: {lang} =========')\n",
    "    t = trange(len(dataloader), desc='ML')\n",
    "    model.eval()\n",
    "    metrics = {'Accuracy': Accuracy(), \n",
    "               'Precision': precision.Precision(average=True), \n",
    "               'Recall': recall.Recall(average=True), \n",
    "               'F1': Fbeta(1)\n",
    "              }\n",
    "    loss_avg = Loss(F.cross_entropy)\n",
    "    for metric in metrics.values():\n",
    "        metric.reset()\n",
    "    loss_avg.reset()\n",
    "    for step, batch in zip(t, dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=input_mask) \n",
    "            emo_loss = F.cross_entropy(outputs.logits, labels, reduction = 'mean')\n",
    "        for metric in metrics.values():\n",
    "            metric.update((outputs.logits, labels.argmax(dim=1))) \n",
    "        loss_avg.update((outputs.logits, labels.argmax(dim=1))) \n",
    "        t.set_description(f'ML (loss={loss_avg.compute():.5f})')\n",
    "    for n, metric in metrics.items():\n",
    "        print(f'   {n}: {metric.compute():.5f}')\n",
    "    print(f'   Loss:     {loss_avg.compute():.5f}')\n",
    "    print(f'==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd4878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('vad_bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d85a04265eabdb4818588eb8176f04cc8800e0221914484e3dd3bf29608e438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
